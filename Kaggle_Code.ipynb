{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-K9j6LQdJLk5"
   },
   "source": [
    "# Google - American Sign Language Fingerspelling Recognition with TensorFlow\n",
    "\n",
    "This notebook walks you through how to train a Transformer model using TensorFlow on the Google - American Sign Language Fingerspelling Recognition dataset made available for this competition.\n",
    "\n",
    "The objective of the model is to predict and translate American Sign Language (ASL) fingerspelling from a set of video frames into text(`phrase`).\n",
    "\n",
    "In this notebook you will learn:\n",
    "\n",
    "- How to load the data\n",
    "- Convert the data to tfrecords to make it faster to re-traing the model\n",
    "- Train a transformer models on the data\n",
    "- Convert the model to TFLite\n",
    "- Create a submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KOoCCQ3EJLk5"
   },
   "source": [
    "# Installation\n",
    "\n",
    "Specifically for this competition, you'll need the mediapipe library to work on the data and visualize it\n",
    "\n",
    "**Note: THE LIBRARIES WILL NOW LOAD!!!!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wcgd9nDJJLk6"
   },
   "source": [
    "# Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "id": "kByNm9wEJLk6"
   },
   "outputs": [],
   "source": [
    "import os                                                  # For handling files and folders on your computer\n",
    "import shutil                                              # For copying, moving, or deleting entire directory trees\n",
    "import numpy as np                                         # For matrices and other math\n",
    "import pandas as pd                                        # For data manipulation and analysis\n",
    "import pyarrow.parquet as pq                               # Brings the parquet module from pyarrow. For reading/writing large datasets\n",
    "import tensorflow as tf                                    # For building and training machine learning models, especially deep neural networks\n",
    "import json                                                # For working with JSON (JavaScript Object Notation) data\n",
    "import mediapipe                                           # For building machine learning pipelines for perception tasks; used to process landmarks\n",
    "import matplotlib                                          # For creating static, animated, and interactive visualizations\n",
    "import matplotlib.pyplot as plt                            # For creating a variety of plots (line plots, scatter plots, histograms, etc.)\n",
    "import random                                              # For pseudo-random number generators\n",
    "\n",
    "from skimage.transform import resize                       # Used to change the dimensions of images\n",
    "from mediapipe.framework.formats import landmark_pb2       # A protobuf definition for landmarks; protobuf is a method of serializing structured data\n",
    "from tensorflow import keras                               # Used to define neural networks\n",
    "from tensorflow.keras import layers                        # Used for neural network layers\n",
    "from tqdm.notebook import tqdm                             # Fancy progress bars; made for Jupiter Notebooks\n",
    "from matplotlib import animation, rc                       # Used to create animations; modifies Matplotlib's default settings\n",
    "\n",
    "print(\"Finished Importing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7vsYxhFTJLk6"
   },
   "outputs": [],
   "source": [
    "print(\"TensorFlow v\" + tf.__version__)\n",
    "print(\"Mediapipe v\" + mediapipe.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ENiaWZ3sJLk6"
   },
   "source": [
    "# Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "In1ShcxDJLk7"
   },
   "outputs": [],
   "source": [
    "dataset_df = pd.read_csv('/kaggle/input/asl-fingerspelling/train.csv')  # Reads data from CSV file\n",
    "print(\"Full train dataset shape is {}\".format(dataset_df.shape))        # Tells us the dimension of the data (n=? for n-tuples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U8zns_5XJLk7"
   },
   "source": [
    "The data is composed of 5 columns and 67208 entries. We can see all 5 dimensions of our dataset by printing out the first 5 entries using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "np2OsGEcJLk7"
   },
   "outputs": [],
   "source": [
    "dataset_df.head()   # Returns the first 5 data points of the data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K4TO4_W8JLk7"
   },
   "source": [
    "# Quick basic dataset exploration\n",
    "\n",
    "Each entry in **train.csv** contains a `phrase`, its `sequence_id`, `path` and `file_id`. The `file_id` indicates the file that holds the **_landmarks_** data for that particular `phrase` and `sequence_id` is the unique index of the landmark sequence within the landmarks data file.\n",
    "\n",
    "The following diagram shows an example of how files are connected.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jCG_D5m9JLk7"
   },
   "source": [
    "As an example, let us examine the landmarks file for the first row of`train.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v7gT4SQNJLk7"
   },
   "outputs": [],
   "source": [
    "# Fetch sequence_id, file_id, phrase from first row\n",
    "sequence_id, file_id, phrase = dataset_df.iloc[0][['sequence_id', 'file_id', 'phrase']]\n",
    "print(f\"sequence_id: {sequence_id}, file_id: {file_id}, phrase: {phrase}\")\n",
    "\n",
    "# Extracts certain parts of the first entry of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vcI4iOklJLk7"
   },
   "source": [
    "Now, let us open the parquet file and fetch the data for the particular `sequence_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G21p8qbyJLk7"
   },
   "outputs": [],
   "source": [
    "# Fetch data from parquet file\n",
    "sample_sequence_df = pq.read_table(f\"/kaggle/input/asl-fingerspelling/train_landmarks/{str(file_id)}.parquet\",\n",
    "    filters=[[('sequence_id', '=', sequence_id)],]).to_pandas()\n",
    "print(\"Full sequence dataset shape is {}\".format(sample_sequence_df.shape))\n",
    "\n",
    "# Loads a specific chunk of landmark data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NocqGcDWJLk7"
   },
   "source": [
    "This particular phrase(***3 creekhouse***) contains 123 entries (or frames) and 1630 columns including 1628 landmark coordinates. Let us print the first 5 entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tw5fiEJ7JLk7"
   },
   "outputs": [],
   "source": [
    "sample_sequence_df.head()      # Returns the first 5 data points of the data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JwOBilW1JLk7"
   },
   "source": [
    "# Data visualization using mediapipe APIs\n",
    "\n",
    "Let us visualize the hand landmarks data for the phrase ***3 creek house*** using the [hand landmarker apis](https://developers.google.com/mediapipe/solutions/vision/hand_landmarker) of the [mediapipe library](https://developers.google.com/mediapipe).\n",
    "\n",
    "Hand landmarks represent the key points on a human hand.\n",
    "\n",
    "Reference: [Data visualization using mediapipe APIs by sknadig](https://www.kaggle.com/code/nadigshreekanth/data-visualization-using-mediapipe-apis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cacasb8qJLk7"
   },
   "outputs": [],
   "source": [
    "# Function create animation from images.\n",
    "\n",
    "matplotlib.rcParams['animation.embed_limit'] = 2**128                      # Adjusting size limit so high quality animations can be created\n",
    "matplotlib.rcParams['savefig.pad_inches'] = 0                              # Padding around image = 0\n",
    "rc('animation', html='jshtml')                                             # Tell the computer to show the animation on a JS player--better than older versions\n",
    "\n",
    "def create_animation(images):                                              # User Defined Function: creates animations\n",
    "    fig = plt.figure(figsize=(6, 9))                                           # Creates a figure, sets the size to 6\"x9\" <-- Animation frame\n",
    "    ax = plt.Axes(fig, [0., 0., 1., 1.])                                       # Creates the axes, links it to the figure, sets the scale\n",
    "    ax.set_axis_off()                                                          # Makes them not visible over the image\n",
    "    fig.add_axes(ax)                                                           # Adds the created axes to the figure\n",
    "    im=ax.imshow(images[0], cmap=\"gray\")                                       # Initializes the first image, applies the colorscheme gray(scale)\n",
    "    plt.close(fig)                                                             # Closes the figure so it doesn't get displayed twice\n",
    "\n",
    "    def animate_func(i):                                                       # User Defined Function: takes specific frames\n",
    "        im.set_array(images[i])                                                    # Updates the pixel data for specific frames\n",
    "        return [im]                                                                # Returns the changes\n",
    "\n",
    "    return animation.FuncAnimation(fig, animate_func, frames=len(images), interval=1000/10)\n",
    "                                                                           # Returns the animation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l_aGIc8-JLk8"
   },
   "source": [
    "This image illustrates the 21 keypoints on the hand.\n",
    "\n",
    "source: https://developers.google.com/mediapipe/solutions/vision/hand_landmarker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ykq9BQUkJLk8"
   },
   "outputs": [],
   "source": [
    "# Extract the landmark data and convert it to an image using mediapipe library.\n",
    "# This function extracts the data for both hands.\n",
    "\n",
    "mp_pose = mediapipe.solutions.pose                                                        # For funcs related to pose estimation\n",
    "mp_hands = mediapipe.solutions.hands                                                      # Logic for hand tracking/detection\n",
    "mp_drawing = mediapipe.solutions.drawing_utils                                            # For drawing hand landmarks\n",
    "mp_drawing_styles = mediapipe.solutions.drawing_styles                                    # Pre-defined styles for drawing landmarks\n",
    "\n",
    "def get_hands(seq_df):                                                                    # User Defined Function: processes frame-by-frame landmark data\n",
    "    images = []                                                                                # Initializes empty list\n",
    "    all_hand_landmarks = []                                                                    # Initializes empty list\n",
    "    for seq_idx in range(len(seq_df)):                                                         # For Loop: iterates through each frame\n",
    "        x_hand = seq_df.iloc[seq_idx].filter(regex=\"x_right_hand.*\").values                    # Extracts all x-coords for the current frame, converts to a numpy array for easier computing\n",
    "        y_hand = seq_df.iloc[seq_idx].filter(regex=\"y_right_hand.*\").values                    # Extracts all y-coords for the current frame, converts to a numpy array for easier computing\n",
    "        z_hand = seq_df.iloc[seq_idx].filter(regex=\"z_right_hand.*\").values                    # Extracts all z-coords for the current frame, converts to a numpy array for easier computing\n",
    "\n",
    "        right_hand_image = np.zeros((600, 600, 3))                                             # Creates an all black image (Numpy image)\n",
    "\n",
    "        right_hand_landmarks = landmark_pb2.NormalizedLandmarkList()                           # Creates empty NormalizedLandmarksList object\n",
    "\n",
    "        for x, y, z in zip(x_hand, y_hand, z_hand):                                            # For Loop: Iterates through all the (x, y, z) tuples\n",
    "            right_hand_landmarks.landmark.add(x=x, y=y, z=z)                                   # Adds each (x, y, z) to the right_hand_landmarks list\n",
    "\n",
    "        mp_drawing.draw_landmarks(                                                             # Draws the landmarks onto the black Numpy image\n",
    "                right_hand_image,                                                              # Image to be drawn on\n",
    "                right_hand_landmarks,                                                          # collection of points to be drawn\n",
    "                mp_hands.HAND_CONNECTIONS,                                                     # pre-define connections between hand landmarks\n",
    "                landmark_drawing_spec=mp_drawing_styles.get_default_hand_landmarks_style())    # visual style\n",
    "\n",
    "        x_hand = seq_df.iloc[seq_idx].filter(regex=\"x_left_hand.*\").values                     # Same thing for left hand\n",
    "        y_hand = seq_df.iloc[seq_idx].filter(regex=\"y_left_hand.*\").values\n",
    "        z_hand = seq_df.iloc[seq_idx].filter(regex=\"z_left_hand.*\").values\n",
    "\n",
    "        left_hand_image = np.zeros((600, 600, 3))\n",
    "\n",
    "        left_hand_landmarks = landmark_pb2.NormalizedLandmarkList()\n",
    "        for x, y, z in zip(x_hand, y_hand, z_hand):\n",
    "            left_hand_landmarks.landmark.add(x=x, y=y, z=z)\n",
    "\n",
    "        mp_drawing.draw_landmarks(\n",
    "                left_hand_image,\n",
    "                left_hand_landmarks,\n",
    "                mp_hands.HAND_CONNECTIONS,\n",
    "                landmark_drawing_spec=mp_drawing_styles.get_default_hand_landmarks_style())\n",
    "\n",
    "        images.append([right_hand_image.astype(np.uint8), left_hand_image.astype(np.uint8)])   # Appends a list containing r/l hand images to the images list\n",
    "        all_hand_landmarks.append([right_hand_landmarks, left_hand_landmarks])                 # Appends MediaPipe landmark objs for hands to the all_hand_landmarks list\n",
    "    return images, all_hand_landmarks                                                     # Returns the images list and all_hand_landmarks list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LWLNXUtJJLk8"
   },
   "outputs": [],
   "source": [
    "# Get the images created using mediapipe apis\n",
    "hand_images, hand_landmarks = get_hands(sample_sequence_df)\n",
    "# Fetch and show the data for right hand\n",
    "create_animation(np.array(hand_images)[:, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hponIhe5JLk8"
   },
   "source": [
    "# Preprocess the data\n",
    "\n",
    "For convenience and efficiency, we will rearrange the data so that each parquet file contains the landmark data along with the phrase it represents. This way we don't have to switch between train.csv and its parquet file.\n",
    "\n",
    "We will save the new data in the [TFRecord](https://www.tensorflow.org/tutorials/load_data/tfrecord) format. `TFRecord` format is a simple format for storing a sequence of binary records. Storing and loading the data using `TFRecord` is much more efficient and faster.\n",
    "\n",
    "Reference:\n",
    "\n",
    "https://www.kaggle.com/code/irohith/aslfr-preprocess-dataset\n",
    "\n",
    "https://www.kaggle.com/code/shlomoron/aslfr-parquets-to-tfrecords-cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_P_1vow5JLk8"
   },
   "source": [
    "ASL-Fingerspelling mainly focusses on hand movement. So we will take hand land mark coordinates and pose coordinates for hands to train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CEzlYvV8JLk8"
   },
   "source": [
    "# LEFT OFF HERE  Fetch the pose landmark coordinates related to hand movement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vNX74zzRJLk8"
   },
   "source": [
    "This image illustrates the pose coordinates related to hand movement. The pose coordinates also come from the parquet file.\n",
    "\n",
    "[Image source](https://developers.google.com/mediapipe/solutions/vision/pose_landmarker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IA5bk64cJLk8"
   },
   "outputs": [],
   "source": [
    "# Pose coordinates for hand movement.\n",
    "LPOSE = [13, 15, 17, 19, 21]\n",
    "RPOSE = [14, 16, 18, 20, 22]\n",
    "POSE = LPOSE + RPOSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m7Wy6LwTJLk8"
   },
   "source": [
    "# Create x,y,z label names from coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L4evfoenJLk8"
   },
   "outputs": [],
   "source": [
    "X = [f'x_right_hand_{i}' for i in range(21)] + [f'x_left_hand_{i}' for i in range(21)] + [f'x_pose_{i}' for i in POSE]\n",
    "Y = [f'y_right_hand_{i}' for i in range(21)] + [f'y_left_hand_{i}' for i in range(21)] + [f'y_pose_{i}' for i in POSE]\n",
    "Z = [f'z_right_hand_{i}' for i in range(21)] + [f'z_left_hand_{i}' for i in range(21)] + [f'z_pose_{i}' for i in POSE]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BKx_6OXeJLk8"
   },
   "source": [
    "Create feature columns from the extracted coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QYXclDmdJLk8"
   },
   "outputs": [],
   "source": [
    "FEATURE_COLUMNS = X + Y + Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HShRL0ruJLk8"
   },
   "source": [
    "Store ids of each coordinate labels to lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PWwPal83JLk8"
   },
   "outputs": [],
   "source": [
    "X_IDX = [i for i, col in enumerate(FEATURE_COLUMNS)  if \"x_\" in col]\n",
    "Y_IDX = [i for i, col in enumerate(FEATURE_COLUMNS)  if \"y_\" in col]\n",
    "Z_IDX = [i for i, col in enumerate(FEATURE_COLUMNS)  if \"z_\" in col]\n",
    "\n",
    "RHAND_IDX = [i for i, col in enumerate(FEATURE_COLUMNS)  if \"right\" in col]\n",
    "LHAND_IDX = [i for i, col in enumerate(FEATURE_COLUMNS)  if  \"left\" in col]\n",
    "RPOSE_IDX = [i for i, col in enumerate(FEATURE_COLUMNS)  if  \"pose\" in col and int(col[-2:]) in RPOSE]\n",
    "LPOSE_IDX = [i for i, col in enumerate(FEATURE_COLUMNS)  if  \"pose\" in col and int(col[-2:]) in LPOSE]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ImA3BSaZJLk8"
   },
   "source": [
    "# Preprocess and write the dataset as TFRecords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BKH0wEJ_JLk8"
   },
   "source": [
    "Using the extracted landmarks and phrases let us create new dataset files and write them as TFRecords.\n",
    "\n",
    "This takes around 10 minutes. After this, loading the dataset will be faster for any future experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "innynQdCJLk8"
   },
   "outputs": [],
   "source": [
    "# Set length of frames to 128\n",
    "FRAME_LEN = 128\n",
    "\n",
    "# Create directory to store the new data\n",
    "if not os.path.isdir(\"preprocessed\"):\n",
    "    os.mkdir(\"preprocessed\")\n",
    "else:\n",
    "    shutil.rmtree(\"preprocessed\")\n",
    "    os.mkdir(\"preprocessed\")\n",
    "\n",
    "# Loop through each file_id\n",
    "for file_id in tqdm(dataset_df.file_id.unique()):\n",
    "    # Parquet file name\n",
    "    pq_file = f\"/kaggle/input/asl-fingerspelling/train_landmarks/{file_id}.parquet\"\n",
    "    # Filter train.csv and fetch entries only for the relevant file_id\n",
    "    file_df = dataset_df.loc[dataset_df[\"file_id\"] == file_id]\n",
    "    # Fetch the parquet file\n",
    "    parquet_df = pq.read_table(f\"/kaggle/input/asl-fingerspelling/train_landmarks/{str(file_id)}.parquet\",\n",
    "                              columns=['sequence_id'] + FEATURE_COLUMNS).to_pandas()\n",
    "    # File name for the updated data\n",
    "    tf_file = f\"preprocessed/{file_id}.tfrecord\"\n",
    "    parquet_numpy = parquet_df.to_numpy()\n",
    "    # Initialize the pointer to write the output of\n",
    "    # each `for loop` below as a sequence into the file.\n",
    "    with tf.io.TFRecordWriter(tf_file) as file_writer:\n",
    "        # Loop through each sequence in file.\n",
    "        for seq_id, phrase in zip(file_df.sequence_id, file_df.phrase):\n",
    "            # Fetch sequence data\n",
    "            frames = parquet_numpy[parquet_df.index == seq_id]\n",
    "\n",
    "            # Calculate the number of NaN values in each hand landmark\n",
    "            r_nonan = np.sum(np.sum(np.isnan(frames[:, RHAND_IDX]), axis = 1) == 0)\n",
    "            l_nonan = np.sum(np.sum(np.isnan(frames[:, LHAND_IDX]), axis = 1) == 0)\n",
    "            no_nan = max(r_nonan, l_nonan)\n",
    "\n",
    "            if 2*len(phrase)<no_nan:\n",
    "                features = {FEATURE_COLUMNS[i]: tf.train.Feature(\n",
    "                    float_list=tf.train.FloatList(value=frames[:, i])) for i in range(len(FEATURE_COLUMNS))}\n",
    "                features[\"phrase\"] = tf.train.Feature(bytes_list=tf.train.BytesList(value=[bytes(phrase, 'utf-8')]))\n",
    "                record_bytes = tf.train.Example(features=tf.train.Features(feature=features)).SerializeToString()\n",
    "                file_writer.write(record_bytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y6fUkFmVJLk8"
   },
   "source": [
    " Load the preprocessed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZRCeWkF6JLk8"
   },
   "source": [
    "# Get the saved TFRecord files into a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "41bAQ8aTJLk8"
   },
   "outputs": [],
   "source": [
    "tf_records = dataset_df.file_id.map(lambda x: f'/kaggle/working/preprocessed/{x}.tfrecord').unique()\n",
    "print(f\"List of {len(tf_records)} TFRecord files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DbuwEHP-JLk_"
   },
   "source": [
    "# Load character_to_prediction json file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9rarvm5aJLk_"
   },
   "source": [
    "This json file contains a character and its value. We will add three new characters, \"<\" and \">\" to mark the start and end of each phrase, and \"P\" for padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AnZUHodTJLk_"
   },
   "outputs": [],
   "source": [
    "with open (\"/kaggle/input/asl-fingerspelling/character_to_prediction_index.json\", \"r\") as f:\n",
    "    char_to_num = json.load(f)\n",
    "\n",
    "# Add pad_token, start pointer and end pointer to the dict\n",
    "pad_token = 'P'\n",
    "start_token = '<'\n",
    "end_token = '>'\n",
    "pad_token_idx = 59\n",
    "start_token_idx = 60\n",
    "end_token_idx = 61\n",
    "\n",
    "char_to_num[pad_token] = pad_token_idx\n",
    "char_to_num[start_token] = start_token_idx\n",
    "char_to_num[end_token] = end_token_idx\n",
    "num_to_char = {j:i for i,j in char_to_num.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n3dt0aDqJLk_"
   },
   "outputs": [],
   "source": [
    "# Reference: https://www.kaggle.com/code/irohith/aslfr-transformer/notebook\n",
    "\n",
    "# Function to resize and add padding.\n",
    "def resize_pad(x):\n",
    "    if tf.shape(x)[0] < FRAME_LEN:\n",
    "        x = tf.pad(x, ([[0, FRAME_LEN-tf.shape(x)[0]], [0, 0], [0, 0]]))\n",
    "    else:\n",
    "        x = tf.image.resize(x, (FRAME_LEN, tf.shape(x)[1]))\n",
    "    return x\n",
    "\n",
    "# Detect the dominant hand from the number of NaN values.\n",
    "# Dominant hand will have less NaN values since it is in frame moving.\n",
    "def pre_process(x):\n",
    "    rhand = tf.gather(x, RHAND_IDX, axis=1)\n",
    "    lhand = tf.gather(x, LHAND_IDX, axis=1)\n",
    "    rpose = tf.gather(x, RPOSE_IDX, axis=1)\n",
    "    lpose = tf.gather(x, LPOSE_IDX, axis=1)\n",
    "\n",
    "    rnan_idx = tf.reduce_any(tf.math.is_nan(rhand), axis=1)\n",
    "    lnan_idx = tf.reduce_any(tf.math.is_nan(lhand), axis=1)\n",
    "\n",
    "    rnans = tf.math.count_nonzero(rnan_idx)\n",
    "    lnans = tf.math.count_nonzero(lnan_idx)\n",
    "\n",
    "    # For dominant hand\n",
    "    if rnans > lnans:\n",
    "        hand = lhand\n",
    "        pose = lpose\n",
    "\n",
    "        hand_x = hand[:, 0*(len(LHAND_IDX)//3) : 1*(len(LHAND_IDX)//3)]\n",
    "        hand_y = hand[:, 1*(len(LHAND_IDX)//3) : 2*(len(LHAND_IDX)//3)]\n",
    "        hand_z = hand[:, 2*(len(LHAND_IDX)//3) : 3*(len(LHAND_IDX)//3)]\n",
    "        hand = tf.concat([1-hand_x, hand_y, hand_z], axis=1)\n",
    "\n",
    "        pose_x = pose[:, 0*(len(LPOSE_IDX)//3) : 1*(len(LPOSE_IDX)//3)]\n",
    "        pose_y = pose[:, 1*(len(LPOSE_IDX)//3) : 2*(len(LPOSE_IDX)//3)]\n",
    "        pose_z = pose[:, 2*(len(LPOSE_IDX)//3) : 3*(len(LPOSE_IDX)//3)]\n",
    "        pose = tf.concat([1-pose_x, pose_y, pose_z], axis=1)\n",
    "    else:\n",
    "        hand = rhand\n",
    "        pose = rpose\n",
    "\n",
    "    hand_x = hand[:, 0*(len(LHAND_IDX)//3) : 1*(len(LHAND_IDX)//3)]\n",
    "    hand_y = hand[:, 1*(len(LHAND_IDX)//3) : 2*(len(LHAND_IDX)//3)]\n",
    "    hand_z = hand[:, 2*(len(LHAND_IDX)//3) : 3*(len(LHAND_IDX)//3)]\n",
    "    hand = tf.concat([hand_x[..., tf.newaxis], hand_y[..., tf.newaxis], hand_z[..., tf.newaxis]], axis=-1)\n",
    "\n",
    "    mean = tf.math.reduce_mean(hand, axis=1)[:, tf.newaxis, :]\n",
    "    std = tf.math.reduce_std(hand, axis=1)[:, tf.newaxis, :]\n",
    "    hand = (hand - mean) / std\n",
    "\n",
    "    pose_x = pose[:, 0*(len(LPOSE_IDX)//3) : 1*(len(LPOSE_IDX)//3)]\n",
    "    pose_y = pose[:, 1*(len(LPOSE_IDX)//3) : 2*(len(LPOSE_IDX)//3)]\n",
    "    pose_z = pose[:, 2*(len(LPOSE_IDX)//3) : 3*(len(LPOSE_IDX)//3)]\n",
    "    pose = tf.concat([pose_x[..., tf.newaxis], pose_y[..., tf.newaxis], pose_z[..., tf.newaxis]], axis=-1)\n",
    "\n",
    "    x = tf.concat([hand, pose], axis=1)\n",
    "    x = resize_pad(x)\n",
    "\n",
    "    x = tf.where(tf.math.is_nan(x), tf.zeros_like(x), x)\n",
    "    x = tf.reshape(x, (FRAME_LEN, len(LHAND_IDX) + len(LPOSE_IDX)))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1x-udXxoJLk_"
   },
   "source": [
    "# Create function to parse data from TFRecord format\n",
    "\n",
    "This function will read the `TFRecord` data and convert it to Tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3N3lN5FZJLk_"
   },
   "outputs": [],
   "source": [
    "def decode_fn(record_bytes):\n",
    "    schema = {COL: tf.io.VarLenFeature(dtype=tf.float32) for COL in FEATURE_COLUMNS}\n",
    "    schema[\"phrase\"] = tf.io.FixedLenFeature([], dtype=tf.string)\n",
    "    features = tf.io.parse_single_example(record_bytes, schema)\n",
    "    phrase = features[\"phrase\"]\n",
    "    landmarks = ([tf.sparse.to_dense(features[COL]) for COL in FEATURE_COLUMNS])\n",
    "    # Transpose to maintain the original shape of landmarks data.\n",
    "    landmarks = tf.transpose(landmarks)\n",
    "\n",
    "    return landmarks, phrase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LW4VqVXLJLk_"
   },
   "source": [
    "# Create function to convert the data\n",
    "\n",
    "This function transposes and applies masks to the landmark coordinates. It also vectorizes the phrase corresponding to the landmarks using `character_to_prediction_index.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ppOEIf5RJLk_"
   },
   "outputs": [],
   "source": [
    "table = tf.lookup.StaticHashTable(\n",
    "    initializer=tf.lookup.KeyValueTensorInitializer(\n",
    "        keys=list(char_to_num.keys()),\n",
    "        values=list(char_to_num.values()),\n",
    "    ),\n",
    "    default_value=tf.constant(-1),\n",
    "    name=\"class_weight\"\n",
    ")\n",
    "\n",
    "def convert_fn(landmarks, phrase):\n",
    "    # Add start and end pointers to phrase.\n",
    "    phrase = start_token + phrase + end_token\n",
    "    phrase = tf.strings.bytes_split(phrase)\n",
    "    phrase = table.lookup(phrase)\n",
    "    # Vectorize and add padding.\n",
    "    phrase = tf.pad(phrase, paddings=[[0, 64 - tf.shape(phrase)[0]]], mode = 'CONSTANT',\n",
    "                    constant_values = pad_token_idx)\n",
    "    # Apply pre_process function to the landmarks.\n",
    "    return pre_process(landmarks), phrase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "noFf5dR5JLk_"
   },
   "source": [
    "\n",
    "\n",
    "Use the functions we defined above to create the final dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4vYfDWI2JLk_"
   },
   "source": [
    "# Train and validation split/Create the final datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3UppV_g5JLlA"
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_len = int(0.8 * len(tf_records))\n",
    "\n",
    "train_ds = tf.data.TFRecordDataset(tf_records[:train_len]).map(decode_fn).map(convert_fn).batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE).cache()\n",
    "valid_ds = tf.data.TFRecordDataset(tf_records[train_len:]).map(decode_fn).map(convert_fn).batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4J_GB4HVJLlA"
   },
   "source": [
    "# Create the Transformer model\n",
    "\n",
    "We will use a **[Transformer](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model))** to train a model for this dataset. **Transformers** are designed to process sequential input data. The model we are going to design is similar to the one used in the [Automatic Speech Recognition with Transformer](https://keras.io/examples/audio/transformer_asr/) tutorial for **Keras**. We will finetune only a small part of the model since we can treat the ASL Fingerspelling recognition problem similar to speech recognition. In both cases, we have to predict a sentence from a sequence of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8WNCCCBaJLlA"
   },
   "source": [
    "# Define the Transformer Input Layers\n",
    "\n",
    "When processing landmark coordinate features for the encoder, we apply convolutional layers to downsample them and process local relationships.\n",
    "\n",
    "We sum position embeddings and token embeddings when processing past target tokens for the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oy8KaaRlJLlA"
   },
   "outputs": [],
   "source": [
    "class TokenEmbedding(layers.Layer):\n",
    "    def __init__(self, num_vocab=1000, maxlen=100, num_hid=64):\n",
    "        super().__init__()\n",
    "        self.emb = tf.keras.layers.Embedding(num_vocab, num_hid)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=num_hid)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        x = self.emb(x)\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        return x + positions\n",
    "\n",
    "\n",
    "class LandmarkEmbedding(layers.Layer):\n",
    "    def __init__(self, num_hid=64, maxlen=100):\n",
    "        super().__init__()\n",
    "        self.conv1 = tf.keras.layers.Conv1D(\n",
    "            num_hid, 11, strides=2, padding=\"same\", activation=\"relu\"\n",
    "        )\n",
    "        self.conv2 = tf.keras.layers.Conv1D(\n",
    "            num_hid, 11, strides=2, padding=\"same\", activation=\"relu\"\n",
    "        )\n",
    "        self.conv3 = tf.keras.layers.Conv1D(\n",
    "            num_hid, 11, strides=2, padding=\"same\", activation=\"relu\"\n",
    "        )\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=num_hid)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        return self.conv3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "katAxDGlJLlA"
   },
   "source": [
    "# Encoder layer for Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GiZhBj9xJLlA"
   },
   "outputs": [],
   "source": [
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, feed_forward_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(feed_forward_dim, activation=\"relu\"),\n",
    "                layers.Dense(embed_dim),\n",
    "            ]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J1KTX8BgJLlA"
   },
   "source": [
    "# Decoder layer for Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1mDsqgJsJLlA"
   },
   "outputs": [],
   "source": [
    "# Customized to add `training` variable\n",
    "# Reference: https://www.kaggle.com/code/shlomoron/aslfr-a-simple-transformer/notebook\n",
    "\n",
    "class TransformerDecoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, feed_forward_dim, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.self_att = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.enc_att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.self_dropout = layers.Dropout(0.5)\n",
    "        self.enc_dropout = layers.Dropout(0.1)\n",
    "        self.ffn_dropout = layers.Dropout(0.1)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(feed_forward_dim, activation=\"relu\"),\n",
    "                layers.Dense(embed_dim),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def causal_attention_mask(self, batch_size, n_dest, n_src, dtype):\n",
    "        \"\"\"Masks the upper half of the dot product matrix in self attention.\n",
    "\n",
    "        This prevents flow of information from future tokens to current token.\n",
    "        1's in the lower triangle, counting from the lower right corner.\n",
    "        \"\"\"\n",
    "        i = tf.range(n_dest)[:, None]\n",
    "        j = tf.range(n_src)\n",
    "        m = i >= j - n_src + n_dest\n",
    "        mask = tf.cast(m, dtype)\n",
    "        mask = tf.reshape(mask, [1, n_dest, n_src])\n",
    "        mult = tf.concat(\n",
    "            [batch_size[..., tf.newaxis], tf.constant([1, 1], dtype=tf.int32)], 0\n",
    "        )\n",
    "        return tf.tile(mask, mult)\n",
    "\n",
    "    def call(self, enc_out, target, training):\n",
    "        input_shape = tf.shape(target)\n",
    "        batch_size = input_shape[0]\n",
    "        seq_len = input_shape[1]\n",
    "        causal_mask = self.causal_attention_mask(batch_size, seq_len, seq_len, tf.bool)\n",
    "        target_att = self.self_att(target, target, attention_mask=causal_mask)\n",
    "        target_norm = self.layernorm1(target + self.self_dropout(target_att, training = training))\n",
    "        enc_out = self.enc_att(target_norm, enc_out)\n",
    "        enc_out_norm = self.layernorm2(self.enc_dropout(enc_out, training = training) + target_norm)\n",
    "        ffn_out = self.ffn(enc_out_norm)\n",
    "        ffn_out_norm = self.layernorm3(enc_out_norm + self.ffn_dropout(ffn_out, training = training))\n",
    "        return ffn_out_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W3S2jxCxJLlA"
   },
   "source": [
    "# Complete the Transformer model\n",
    "\n",
    "This model takes landmark coordinates as inputs and predicts a sequence of characters. The target character sequence, which has been shifted to the left is provided as the input to the decoder during training. The decoder employs its own past predictions during inference to forecast the next token.\n",
    "\n",
    "The **Levenshtein Distance** between sequences is used as the accuracy metric since the evaluation metric for this contest is the **Normalized Total Levenshtein Distance**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iRM4FW8PJLlA"
   },
   "outputs": [],
   "source": [
    "# Customized to add edit_dist metric and training variable.\n",
    "# Reference:\n",
    "# https://www.kaggle.com/code/irohith/aslfr-transformer/notebook\n",
    "# https://www.kaggle.com/code/shlomoron/aslfr-a-simple-transformer/notebook\n",
    "\n",
    "class Transformer(keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_hid=64,\n",
    "        num_head=2,\n",
    "        num_feed_forward=128,\n",
    "        source_maxlen=100,\n",
    "        target_maxlen=100,\n",
    "        num_layers_enc=4,\n",
    "        num_layers_dec=1,\n",
    "        num_classes=60,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.loss_metric = keras.metrics.Mean(name=\"loss\")\n",
    "        self.acc_metric = keras.metrics.Mean(name=\"edit_dist\")\n",
    "        self.num_layers_enc = num_layers_enc\n",
    "        self.num_layers_dec = num_layers_dec\n",
    "        self.target_maxlen = target_maxlen\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.enc_input = LandmarkEmbedding(num_hid=num_hid, maxlen=source_maxlen)\n",
    "        self.dec_input = TokenEmbedding(\n",
    "            num_vocab=num_classes, maxlen=target_maxlen, num_hid=num_hid\n",
    "        )\n",
    "\n",
    "        self.encoder = keras.Sequential(\n",
    "            [self.enc_input]\n",
    "            + [\n",
    "                TransformerEncoder(num_hid, num_head, num_feed_forward)\n",
    "                for _ in range(num_layers_enc)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        for i in range(num_layers_dec):\n",
    "            setattr(\n",
    "                self,\n",
    "                f\"dec_layer_{i}\",\n",
    "                TransformerDecoder(num_hid, num_head, num_feed_forward),\n",
    "            )\n",
    "\n",
    "        self.classifier = layers.Dense(num_classes)\n",
    "\n",
    "    def decode(self, enc_out, target, training):\n",
    "        y = self.dec_input(target)\n",
    "        for i in range(self.num_layers_dec):\n",
    "            y = getattr(self, f\"dec_layer_{i}\")(enc_out, y, training)\n",
    "        return y\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        source = inputs[0]\n",
    "        target = inputs[1]\n",
    "        x = self.encoder(source, training)\n",
    "        y = self.decode(x, target, training)\n",
    "        return self.classifier(y)\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.loss_metric]\n",
    "\n",
    "    def train_step(self, batch):\n",
    "        \"\"\"Processes one batch inside model.fit().\"\"\"\n",
    "        source = batch[0]\n",
    "        target = batch[1]\n",
    "\n",
    "        input_shape = tf.shape(target)\n",
    "        batch_size = input_shape[0]\n",
    "\n",
    "        dec_input = target[:, :-1]\n",
    "        dec_target = target[:, 1:]\n",
    "        with tf.GradientTape() as tape:\n",
    "            preds = self([source, dec_input])\n",
    "            one_hot = tf.one_hot(dec_target, depth=self.num_classes)\n",
    "            mask = tf.math.logical_not(tf.math.equal(dec_target, pad_token_idx))\n",
    "            loss = self.compiled_loss(one_hot, preds, sample_weight=mask)\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        # Computes the Levenshtein distance between sequences since the evaluation\n",
    "        # metric for this contest is the normalized total levenshtein distance.\n",
    "        edit_dist = tf.edit_distance(tf.sparse.from_dense(target),\n",
    "                                     tf.sparse.from_dense(tf.cast(tf.argmax(preds, axis=1), tf.int32)))\n",
    "        edit_dist = tf.reduce_mean(edit_dist)\n",
    "        self.acc_metric.update_state(edit_dist)\n",
    "        self.loss_metric.update_state(loss)\n",
    "        return {\"loss\": self.loss_metric.result(), \"edit_dist\": self.acc_metric.result()}\n",
    "\n",
    "    def test_step(self, batch):\n",
    "        source = batch[0]\n",
    "        target = batch[1]\n",
    "\n",
    "        input_shape = tf.shape(target)\n",
    "        batch_size = input_shape[0]\n",
    "\n",
    "        dec_input = target[:, :-1]\n",
    "        dec_target = target[:, 1:]\n",
    "        preds = self([source, dec_input])\n",
    "        one_hot = tf.one_hot(dec_target, depth=self.num_classes)\n",
    "        mask = tf.math.logical_not(tf.math.equal(dec_target, pad_token_idx))\n",
    "        loss = self.compiled_loss(one_hot, preds, sample_weight=mask)\n",
    "        # Computes the Levenshtein distance between sequences since the evaluation\n",
    "        # metric for this contest is the normalized total levenshtein distance.\n",
    "        edit_dist = tf.edit_distance(tf.sparse.from_dense(target),\n",
    "                                     tf.sparse.from_dense(tf.cast(tf.argmax(preds, axis=1), tf.int32)))\n",
    "        edit_dist = tf.reduce_mean(edit_dist)\n",
    "        self.acc_metric.update_state(edit_dist)\n",
    "        self.loss_metric.update_state(loss)\n",
    "        return {\"loss\": self.loss_metric.result(), \"edit_dist\": self.acc_metric.result()}\n",
    "\n",
    "    def generate(self, source, target_start_token_idx):\n",
    "        \"\"\"Performs inference over one batch of inputs using greedy decoding.\"\"\"\n",
    "        bs = tf.shape(source)[0]\n",
    "        enc = self.encoder(source, training = False)\n",
    "        dec_input = tf.ones((bs, 1), dtype=tf.int32) * target_start_token_idx\n",
    "        dec_logits = []\n",
    "        for i in range(self.target_maxlen - 1):\n",
    "            dec_out = self.decode(enc, dec_input, training = False)\n",
    "            logits = self.classifier(dec_out)\n",
    "            logits = tf.argmax(logits, axis=-1, output_type=tf.int32)\n",
    "            last_logit = logits[:, -1][..., tf.newaxis]\n",
    "            dec_logits.append(last_logit)\n",
    "            dec_input = tf.concat([dec_input, last_logit], axis=-1)\n",
    "        return dec_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DjRvoeUbJLlA"
   },
   "source": [
    "The following callback function is used to display predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v0OicXqdJLlA"
   },
   "outputs": [],
   "source": [
    "class DisplayOutputs(keras.callbacks.Callback):\n",
    "    def __init__(\n",
    "        self, batch, idx_to_token, target_start_token_idx=60, target_end_token_idx=61\n",
    "    ):\n",
    "        \"\"\"Displays a batch of outputs after every 4 epoch\n",
    "\n",
    "        Args:\n",
    "            batch: A test batch\n",
    "            idx_to_token: A List containing the vocabulary tokens corresponding to their indices\n",
    "            target_start_token_idx: A start token index in the target vocabulary\n",
    "            target_end_token_idx: An end token index in the target vocabulary\n",
    "        \"\"\"\n",
    "        self.batch = batch\n",
    "        self.target_start_token_idx = target_start_token_idx\n",
    "        self.target_end_token_idx = target_end_token_idx\n",
    "        self.idx_to_char = idx_to_token\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch % 4 != 0:\n",
    "            return\n",
    "        source = self.batch[0]\n",
    "        target = self.batch[1].numpy()\n",
    "        bs = tf.shape(source)[0]\n",
    "        preds = self.model.generate(source, self.target_start_token_idx)\n",
    "        preds = preds.numpy()\n",
    "        for i in range(bs):\n",
    "            target_text = \"\".join([self.idx_to_char[_] for _ in target[i, :]])\n",
    "            prediction = \"\"\n",
    "            for idx in preds[i, :]:\n",
    "                prediction += self.idx_to_char[idx]\n",
    "                if idx == self.target_end_token_idx:\n",
    "                    break\n",
    "            print(f\"target:     {target_text.replace('-','')}\")\n",
    "            print(f\"prediction: {prediction}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PmlB-19rJLlA"
   },
   "source": [
    "# Train the Transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b9gwy-FUJLlA"
   },
   "outputs": [],
   "source": [
    "# Transformer variables are customized from original keras tutorial to suit this dataset.\n",
    "# Reference: https://www.kaggle.com/code/shlomoron/aslfr-a-simple-transformer/notebook\n",
    "\n",
    "batch = next(iter(valid_ds))\n",
    "\n",
    "# The vocabulary to convert predicted indices into characters\n",
    "idx_to_char = list(char_to_num.keys())\n",
    "display_cb = DisplayOutputs(\n",
    "    batch, idx_to_char, target_start_token_idx=char_to_num['<'], target_end_token_idx=char_to_num['>']\n",
    ")  # set the arguments as per vocabulary index for '<' and '>'\n",
    "\n",
    "model = Transformer(\n",
    "    num_hid=200,\n",
    "    num_head=4,\n",
    "    num_feed_forward=400,\n",
    "    source_maxlen = FRAME_LEN,\n",
    "    target_maxlen=64,\n",
    "    num_layers_enc=2,\n",
    "    num_layers_dec=1,\n",
    "    num_classes=62\n",
    ")\n",
    "loss_fn = tf.keras.losses.CategoricalCrossentropy(\n",
    "    from_logits=True, label_smoothing=0.1,\n",
    ")\n",
    "\n",
    "\n",
    "optimizer = keras.optimizers.Adam(0.0001)\n",
    "model.compile(optimizer=optimizer, loss=loss_fn)\n",
    "\n",
    "history = model.fit(train_ds, validation_data=valid_ds, callbacks=[display_cb], epochs=13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rFWQMdwXJLlA"
   },
   "source": [
    "# Plot training loss and validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WY3DQCaJJLlA"
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.legend(['training loss', 'val_loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pUFz2FXqJLlA"
   },
   "source": [
    "# Submission\n",
    "\n",
    "Refrence: https://www.kaggle.com/code/shlomoron/aslfr-a-simple-transformer/notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pWWb8aRVJLlA"
   },
   "source": [
    "# Create TFLite model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Khr3ADnfJLlA"
   },
   "outputs": [],
   "source": [
    " class TFLiteModel(tf.Module):\n",
    "    def __init__(self, model):\n",
    "        super(TFLiteModel, self).__init__()\n",
    "        self.target_start_token_idx = start_token_idx\n",
    "        self.target_end_token_idx = end_token_idx\n",
    "        # Load the feature generation and main models\n",
    "        self.model = model\n",
    "\n",
    "    @tf.function(input_signature=[tf.TensorSpec(shape=[None, len(FEATURE_COLUMNS)], dtype=tf.float32, name='inputs')])\n",
    "    def __call__(self, inputs, training=False):\n",
    "        # Preprocess Data\n",
    "        x = tf.cast(inputs, tf.float32)\n",
    "        x = x[None]\n",
    "        x = tf.cond(tf.shape(x)[1] == 0, lambda: tf.zeros((1, 1, len(FEATURE_COLUMNS))), lambda: tf.identity(x))\n",
    "        x = x[0]\n",
    "        x = pre_process(x)\n",
    "        x = x[None]\n",
    "        x = self.model.generate(x, self.target_start_token_idx)\n",
    "        x = x[0]\n",
    "        idx = tf.argmax(tf.cast(tf.equal(x, self.target_end_token_idx), tf.int32))\n",
    "        idx = tf.where(tf.math.less(idx, 1), tf.constant(2, dtype=tf.int64), idx)\n",
    "        x = x[1:idx]\n",
    "        x = tf.one_hot(x, 59)\n",
    "        return {'outputs': x}\n",
    "\n",
    "tflitemodel_base = TFLiteModel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qfXYRP0NJLlA"
   },
   "outputs": [],
   "source": [
    "model.save_weights(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fu91zTewJLlB"
   },
   "outputs": [],
   "source": [
    "keras_model_converter = tf.lite.TFLiteConverter.from_keras_model(tflitemodel_base)\n",
    "keras_model_converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]#, tf.lite.OpsSet.SELECT_TF_OPS]\n",
    "tflite_model = keras_model_converter.convert()\n",
    "with open('/kaggle/working/model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "infargs = {\"selected_columns\" : FEATURE_COLUMNS}\n",
    "\n",
    "with open('inference_args.json', \"w\") as json_file:\n",
    "    json.dump(infargs, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qj2rXWM1JLlB"
   },
   "outputs": [],
   "source": [
    "!zip submission.zip  './model.tflite' './inference_args.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ljPQLRy5JLlB"
   },
   "outputs": [],
   "source": [
    "interpreter = tf.lite.Interpreter(\"model.tflite\")\n",
    "\n",
    "REQUIRED_SIGNATURE = \"serving_default\"\n",
    "REQUIRED_OUTPUT = \"outputs\"\n",
    "\n",
    "with open (\"/kaggle/input/asl-fingerspelling/character_to_prediction_index.json\", \"r\") as f:\n",
    "    character_map = json.load(f)\n",
    "rev_character_map = {j:i for i,j in character_map.items()}\n",
    "\n",
    "found_signatures = list(interpreter.get_signature_list().keys())\n",
    "\n",
    "if REQUIRED_SIGNATURE not in found_signatures:\n",
    "    raise KernelEvalException('Required input signature not found.')\n",
    "\n",
    "prediction_fn = interpreter.get_signature_runner(\"serving_default\")\n",
    "output = prediction_fn(inputs=batch[0][0])\n",
    "prediction_str = \"\".join([rev_character_map.get(s, \"\") for s in np.argmax(output[REQUIRED_OUTPUT], axis=1)])\n",
    "print(prediction_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mediapipe\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import tensorflow as tf\n",
    "import json\n",
    "import mediapipe\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from skimage.transform import resize\n",
    "# UPDATED: Import NormalizedLandmarkList from the new location\n",
    "from mediapipe.tasks.python.components.containers import NormalizedLandmarkList\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tqdm.notebook import tqdm\n",
    "from matplotlib import animation, rc\n",
    "\n",
    "print(\"TensorFlow v\" + tf.__version__)\n",
    "print(\"Mediapipe v\" + mediapipe.__version__)\n",
    "\n",
    "dataset_df = pd.read_csv('/kaggle/input/asl-fingerspelling/train.csv')\n",
    "print(\"Full train dataset shape is {}\".format(dataset_df.shape))\n",
    "\n",
    "dataset_df.head()\n",
    "\n",
    "# Fetch sequence_id, file_id, phrase from first row\n",
    "sequence_id, file_id, phrase = dataset_df.iloc[0][['sequence_id', 'file_id', 'phrase']]\n",
    "print(f\"sequence_id: {sequence_id}, file_id: {file_id}, phrase: {phrase}\")\n",
    "\n",
    "# Fetch data from parquet file\n",
    "sample_sequence_df = pq.read_table(f\"/kaggle/input/asl-fingerspelling/train_landmarks/{str(file_id)}.parquet\",\n",
    "    filters=[[('sequence_id', '=', sequence_id)],]).to_pandas()\n",
    "print(\"Full sequence dataset shape is {}\".format(sample_sequence_df.shape))\n",
    "\n",
    "sample_sequence_df.head()\n",
    "\n",
    "# Function create animation from images.\n",
    "\n",
    "matplotlib.rcParams['animation.embed_limit'] = 2**128\n",
    "matplotlib.rcParams['savefig.pad_inches'] = 0\n",
    "rc('animation', html='jshtml')\n",
    "\n",
    "def create_animation(images):\n",
    "    fig = plt.figure(figsize=(6, 9))\n",
    "    ax = plt.Axes(fig, [0., 0., 1., 1.])\n",
    "    ax.set_axis_off()\n",
    "    fig.add_axes(ax)\n",
    "    im=ax.imshow(images[0], cmap=\"gray\")\n",
    "    plt.close(fig)\n",
    "\n",
    "    def animate_func(i):\n",
    "        im.set_array(images[i])\n",
    "        return [im]\n",
    "\n",
    "    return animation.FuncAnimation(fig, animate_func, frames=len(images), interval=1000/10)\n",
    "\n",
    "# Extract the landmark data and convert it to an image using medipipe library.\n",
    "# This function extracts the data for both hands.\n",
    "\n",
    "mp_pose = mediapipe.solutions.pose\n",
    "mp_hands = mediapipe.solutions.hands\n",
    "mp_drawing = mediapipe.solutions.drawing_utils\n",
    "mp_drawing_styles = mediapipe.solutions.drawing_styles\n",
    "\n",
    "def get_hands(seq_df):\n",
    "    images = []\n",
    "    all_hand_landmarks = []\n",
    "    for seq_idx in range(len(seq_df)):\n",
    "        x_hand = seq_df.iloc[seq_idx].filter(regex=\"x_right_hand.*\").values\n",
    "        y_hand = seq_df.iloc[seq_idx].filter(regex=\"y_right_hand.*\").values\n",
    "        z_hand = seq_df.iloc[seq_idx].filter(regex=\"z_right_hand.*\").values\n",
    "\n",
    "        right_hand_image = np.zeros((600, 600, 3))\n",
    "\n",
    "        # UPDATED: Use NormalizedLandmarkList directly\n",
    "        right_hand_landmarks = NormalizedLandmarkList()\n",
    "\n",
    "        for x, y, z in zip(x_hand, y_hand, z_hand):\n",
    "            right_hand_landmarks.landmark.add(x=x, y=y, z=z)\n",
    "\n",
    "        mp_drawing.draw_landmarks(\n",
    "                right_hand_image,\n",
    "                right_hand_landmarks,\n",
    "                mp_hands.HAND_CONNECTIONS,\n",
    "                landmark_drawing_spec=mp_drawing_styles.get_default_hand_landmarks_style())\n",
    "\n",
    "        x_hand = seq_df.iloc[seq_idx].filter(regex=\"x_left_hand.*\").values\n",
    "        y_hand = seq_df.iloc[seq_idx].filter(regex=\"y_left_hand.*\").values\n",
    "        z_hand = seq_df.iloc[seq_idx].filter(regex=\"z_left_hand.*\").values\n",
    "\n",
    "        left_hand_image = np.zeros((600, 600, 3))\n",
    "\n",
    "        # UPDATED: Use NormalizedLandmarkList directly\n",
    "        left_hand_landmarks = NormalizedLandmarkList()\n",
    "        for x, y, z in zip(x_hand, y_hand, z_hand):\n",
    "            left_hand_landmarks.landmark.add(x=x, y=y, z=z)\n",
    "\n",
    "        mp_drawing.draw_landmarks(\n",
    "                left_hand_image,\n",
    "                left_hand_landmarks,\n",
    "                mp_hands.HAND_CONNECTIONS,\n",
    "                landmark_drawing_spec=mp_drawing_styles.get_default_hand_landmarks_style())\n",
    "\n",
    "        images.append([right_hand_image.astype(np.uint8), left_hand_image.astype(np.uint8)])\n",
    "        all_hand_landmarks.append([right_hand_landmarks, left_hand_landmarks])\n",
    "    return images, all_hand_landmarks\n",
    "\n",
    "# Get the images created using mediapipe apis\n",
    "hand_images, hand_landmarks = get_hands(sample_sequence_df)\n",
    "# Fetch and show the data for right hand\n",
    "create_animation(np.array(hand_images)[:, 0])\n",
    "\n",
    "# Pose coordinates for hand movement.\n",
    "LPOSE = [13, 15, 17, 19, 21]\n",
    "RPOSE = [14, 16, 18, 20, 22]\n",
    "POSE = LPOSE + RPOSE\n",
    "\n",
    "X = [f'x_right_hand_{i}' for i in range(21)] + [f'x_left_hand_{i}' for i in range(21)] + [f'x_pose_{i}' for i in POSE]\n",
    "Y = [f'y_right_hand_{i}' for i in range(21)] + [f'y_left_hand_{i}' for i in range(21)] + [f'y_pose_{i}' for i in POSE]\n",
    "Z = [f'z_right_hand_{i}' for i in range(21)] + [f'z_left_hand_{i}' for i in range(21)] + [f'z_pose_{i}' for i in POSE]\n",
    "\n",
    "FEATURE_COLUMNS = X + Y + Z\n",
    "\n",
    "X_IDX = [i for i, col in enumerate(FEATURE_COLUMNS)  if \"x_\" in col]\n",
    "Y_IDX = [i for i, col in enumerate(FEATURE_COLUMNS)  if \"y_\" in col]\n",
    "Z_IDX = [i for i, col in enumerate(FEATURE_COLUMNS)  if \"z_\" in col]\n",
    "\n",
    "RHAND_IDX = [i for i, col in enumerate(FEATURE_COLUMNS)  if \"right\" in col]\n",
    "LHAND_IDX = [i for i, col in enumerate(FEATURE_COLUMNS)  if  \"left\" in col]\n",
    "RPOSE_IDX = [i for i, col in enumerate(FEATURE_COLUMNS)  if  \"pose\" in col and int(col[-2:]) in RPOSE]\n",
    "LPOSE_IDX = [i for i, col in enumerate(FEATURE_COLUMNS)  if  \"pose\" in col and int(col[-2:]) in LPOSE]\n",
    "\n",
    "# Set length of frames to 128\n",
    "FRAME_LEN = 128\n",
    "\n",
    "# Create directory to store the new data\n",
    "if not os.path.isdir(\"preprocessed\"):\n",
    "    os.mkdir(\"preprocessed\")\n",
    "else:\n",
    "    shutil.rmtree(\"preprocessed\")\n",
    "    os.mkdir(\"preprocessed\")\n",
    "\n",
    "# Loop through each file_id\n",
    "for file_id in tqdm(dataset_df.file_id.unique()):\n",
    "    # Parquet file name\n",
    "    pq_file = f\"/kaggle/input/asl-fingerspelling/train_landmarks/{file_id}.parquet\"\n",
    "    # Filter train.csv and fetch entries only for the relevant file_id\n",
    "    file_df = dataset_df.loc[dataset_df[\"file_id\"] == file_id]\n",
    "    # Fetch the parquet file\n",
    "    parquet_df = pq.read_table(f\"/kaggle/input/asl-fingerspelling/train_landmarks/{str(file_id)}.parquet\",\n",
    "                              columns=['sequence_id'] + FEATURE_COLUMNS).to_pandas()\n",
    "    # File name for the updated data\n",
    "    tf_file = f\"preprocessed/{file_id}.tfrecord\"\n",
    "    parquet_numpy = parquet_df.to_numpy()\n",
    "    # Initialize the pointer to write the output of\n",
    "    # each `for loop` below as a sequence into the file.\n",
    "    with tf.io.TFRecordWriter(tf_file) as file_writer:\n",
    "        # Loop through each sequence in file.\n",
    "        for seq_id, phrase in zip(file_df.sequence_id, file_df.phrase):\n",
    "            # Fetch sequence data\n",
    "            frames = parquet_numpy[parquet_df.index == seq_id]\n",
    "\n",
    "            # Calculate the number of NaN values in each hand landmark\n",
    "            r_nonan = np.sum(np.sum(np.isnan(frames[:, RHAND_IDX]), axis = 1) == 0)\n",
    "            l_nonan = np.sum(np.sum(np.isnan(frames[:, LHAND_IDX]), axis = 1) == 0)\n",
    "            no_nan = max(r_nonan, l_nonan)\n",
    "\n",
    "            if 2*len(phrase)<no_nan:\n",
    "                features = {FEATURE_COLUMNS[i]: tf.train.Feature(\n",
    "                    float_list=tf.train.FloatList(value=frames[:, i])) for i in range(len(FEATURE_COLUMNS))}\n",
    "                features[\"phrase\"] = tf.train.Feature(bytes_list=tf.train.BytesList(value=[bytes(phrase, 'utf-8')]))\n",
    "                record_bytes = tf.train.Example(features=tf.train.Features(feature=features)).SerializeToString()\n",
    "                file_writer.write(record_bytes)\n",
    "\n",
    "tf_records = dataset_df.file_id.map(lambda x: f'/kaggle/working/preprocessed/{x}.tfrecord').unique()\n",
    "print(f\"List of {len(tf_records)} TFRecord files.\")\n",
    "\n",
    "with open (\"/kaggle/input/asl-fingerspelling/character_to_prediction_index.json\", \"r\") as f:\n",
    "    char_to_num = json.load(f)\n",
    "\n",
    "# Add pad_token, start pointer and end pointer to the dict\n",
    "pad_token = 'P'\n",
    "start_token = '<'\n",
    "end_token = '>'\n",
    "pad_token_idx = 59\n",
    "start_token_idx = 60\n",
    "end_token_idx = 61\n",
    "\n",
    "char_to_num[pad_token] = pad_token_idx\n",
    "char_to_num[start_token] = start_token_idx\n",
    "char_to_num[end_token] = end_token_idx\n",
    "num_to_char = {j:i for i,j in char_to_num.items()}\n",
    "\n",
    "# Reference: https://www.kaggle.com/code/irohith/aslfr-transformer/notebook\n",
    "\n",
    "# Function to resize and add padding.\n",
    "def resize_pad(x):\n",
    "    if tf.shape(x)[0] < FRAME_LEN:\n",
    "        x = tf.pad(x, ([[0, FRAME_LEN-tf.shape(x)[0]], [0, 0], [0, 0]]))\n",
    "    else:\n",
    "        x = tf.image.resize(x, (FRAME_LEN, tf.shape(x)[1]))\n",
    "    return x\n",
    "\n",
    "# Detect the dominant hand from the number of NaN values.\n",
    "# Dominant hand will have less NaN values since it is in frame moving.\n",
    "def pre_process(x):\n",
    "    rhand = tf.gather(x, RHAND_IDX, axis=1)\n",
    "    lhand = tf.gather(x, LHAND_IDX, axis=1)\n",
    "    rpose = tf.gather(x, RPOSE_IDX, axis=1)\n",
    "    lpose = tf.gather(x, LPOSE_IDX, axis=1)\n",
    "\n",
    "    rnan_idx = tf.reduce_any(tf.math.is_nan(rhand), axis=1)\n",
    "    lnan_idx = tf.reduce_any(tf.math.is_nan(lhand), axis=1)\n",
    "\n",
    "    rnans = tf.math.count_nonzero(rnan_idx)\n",
    "    lnans = tf.math.count_nonzero(lnan_idx)\n",
    "\n",
    "    # For dominant hand\n",
    "    if rnans > lnans:\n",
    "        hand = lhand\n",
    "        pose = lpose\n",
    "\n",
    "        hand_x = hand[:, 0*(len(LHAND_IDX)//3) : 1*(len(LHAND_IDX)//3)]\n",
    "        hand_y = hand[:, 1*(len(LHAND_IDX)//3) : 2*(len(LHAND_IDX)//3)]\n",
    "        hand_z = hand[:, 2*(len(LHAND_IDX)//3) : 3*(len(LHAND_IDX)//3)]\n",
    "        hand = tf.concat([1-hand_x, hand_y, hand_z], axis=1)\n",
    "\n",
    "        pose_x = pose[:, 0*(len(LPOSE_IDX)//3) : 1*(len(LPOSE_IDX)//3)]\n",
    "        pose_y = pose[:, 1*(len(LPOSE_IDX)//3) : 2*(len(LPOSE_IDX)//3)]\n",
    "        pose_z = pose[:, 2*(len(LPOSE_IDX)//3) : 3*(len(LPOSE_IDX)//3)]\n",
    "        pose = tf.concat([1-pose_x, pose_y, pose_z], axis=1)\n",
    "    else:\n",
    "        hand = rhand\n",
    "        pose = rpose\n",
    "\n",
    "    hand_x = hand[:, 0*(len(LHAND_IDX)//3) : 1*(len(LHAND_IDX)//3)]\n",
    "    hand_y = hand[:, 1*(len(LHAND_IDX)//3) : 2*(len(LHAND_IDX)//3)]\n",
    "    hand_z = hand[:, 2*(len(LHAND_IDX)//3) : 3*(len(LHAND_IDX)//3)]\n",
    "    hand = tf.concat([hand_x[..., tf.newaxis], hand_y[..., tf.newaxis], hand_z[..., tf.newaxis]], axis=-1)\n",
    "\n",
    "    mean = tf.math.reduce_mean(hand, axis=1)[:, tf.newaxis, :]\n",
    "    std = tf.math.reduce_std(hand, axis=1)[:, tf.newaxis, :]\n",
    "    hand = (hand - mean) / std\n",
    "\n",
    "    pose_x = pose[:, 0*(len(LPOSE_IDX)//3) : 1*(len(LPOSE_IDX)//3)]\n",
    "    pose_y = pose[:, 1*(len(LPOSE_IDX)//3) : 2*(len(LPOSE_IDX)//3)]\n",
    "    pose_z = pose[:, 2*(len(LPOSE_IDX)//3) : 3*(len(LPOSE_IDX)//3)]\n",
    "    pose = tf.concat([pose_x[..., tf.newaxis], pose_y[..., tf.newaxis], pose_z[..., tf.newaxis]], axis=-1)\n",
    "\n",
    "    x = tf.concat([hand, pose], axis=1)\n",
    "    x = resize_pad(x)\n",
    "\n",
    "    x = tf.where(tf.math.is_nan(x), tf.zeros_like(x), x)\n",
    "    x = tf.reshape(x, (FRAME_LEN, len(LHAND_IDX) + len(LPOSE_IDX)))\n",
    "    return x\n",
    "\n",
    "def decode_fn(record_bytes):\n",
    "    schema = {COL: tf.io.VarLenFeature(dtype=tf.float32) for COL in FEATURE_COLUMNS}\n",
    "    schema[\"phrase\"] = tf.io.FixedLenFeature([], dtype=tf.string)\n",
    "    features = tf.io.parse_single_example(record_bytes, schema)\n",
    "    phrase = features[\"phrase\"]\n",
    "    landmarks = ([tf.sparse.to_dense(features[COL]) for COL in FEATURE_COLUMNS])\n",
    "    # Transpose to maintain the original shape of landmarks data.\n",
    "    landmarks = tf.transpose(landmarks)\n",
    "\n",
    "    return landmarks, phrase\n",
    "\n",
    "table = tf.lookup.StaticHashTable(\n",
    "    initializer=tf.lookup.KeyValueTensorInitializer(\n",
    "        keys=list(char_to_num.keys()),\n",
    "        values=list(char_to_num.values()),\n",
    "    ),\n",
    "    default_value=tf.constant(-1),\n",
    "    name=\"class_weight\"\n",
    ")\n",
    "\n",
    "def convert_fn(landmarks, phrase):\n",
    "    # Add start and end pointers to phrase.\n",
    "    phrase = start_token + phrase + end_token\n",
    "    phrase = tf.strings.bytes_split(phrase)\n",
    "    phrase = table.lookup(phrase)\n",
    "    # Vectorize and add padding.\n",
    "    phrase = tf.pad(phrase, paddings=[[0, 64 - tf.shape(phrase)[0]]], mode = 'CONSTANT',\n",
    "                     constant_values = pad_token_idx)\n",
    "    # Apply pre_process function to the landmarks.\n",
    "    return pre_process(landmarks), phrase\n",
    "\n",
    "batch_size = 64\n",
    "train_len = int(0.8 * len(tf_records))\n",
    "\n",
    "train_ds = tf.data.TFRecordDataset(tf_records[:train_len]).map(decode_fn).map(convert_fn).batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE).cache()\n",
    "valid_ds = tf.data.TFRecordDataset(tf_records[train_len:]).map(decode_fn).map(convert_fn).batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE).cache()\n",
    "\n",
    "class TokenEmbedding(layers.Layer):\n",
    "    def __init__(self, num_vocab=1000, maxlen=100, num_hid=64):\n",
    "        super().__init__()\n",
    "        self.emb = tf.keras.layers.Embedding(num_vocab, num_hid)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=num_hid)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        x = self.emb(x)\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        return x + positions\n",
    "\n",
    "\n",
    "class LandmarkEmbedding(layers.Layer):\n",
    "    def __init__(self, num_hid=64, maxlen=100):\n",
    "        super().__init__()\n",
    "        self.conv1 = tf.keras.layers.Conv1D(\n",
    "            num_hid, 11, strides=2, padding=\"same\", activation=\"relu\"\n",
    "        )\n",
    "        self.conv2 = tf.keras.layers.Conv1D(\n",
    "            num_hid, 11, strides=2, padding=\"same\", activation=\"relu\"\n",
    "        )\n",
    "        self.conv3 = tf.keras.layers.Conv1D(\n",
    "            num_hid, 11, strides=2, padding=\"same\", activation=\"relu\"\n",
    "        )\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=num_hid)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        return self.conv3(x)\n",
    "\n",
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, feed_forward_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(feed_forward_dim, activation=\"relu\"),\n",
    "                layers.Dense(embed_dim),\n",
    "            ]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "# Customized to add `training` variable\n",
    "# Reference: https://www.kaggle.com/code/shlomoron/aslfr-a-simple-transformer/notebook\n",
    "\n",
    "class TransformerDecoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, feed_forward_dim, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.self_att = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.enc_att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.self_dropout = layers.Dropout(0.5)\n",
    "        self.enc_dropout = layers.Dropout(0.1)\n",
    "        self.ffn_dropout = layers.Dropout(0.1)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(feed_forward_dim, activation=\"relu\"),\n",
    "                layers.Dense(embed_dim),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def causal_attention_mask(self, batch_size, n_dest, n_src, dtype):\n",
    "        \"\"\"Masks the upper half of the dot product matrix in self attention.\n",
    "\n",
    "        This prevents flow of information from future tokens to current token.\n",
    "        1's in the lower triangle, counting from the lower right corner.\n",
    "        \"\"\"\n",
    "        i = tf.range(n_dest)[:, None]\n",
    "        j = tf.range(n_src)\n",
    "        m = i >= j - n_src + n_dest\n",
    "        mask = tf.cast(m, dtype)\n",
    "        mask = tf.reshape(mask, [1, n_dest, n_src])\n",
    "        mult = tf.concat(\n",
    "            [batch_size[..., tf.newaxis], tf.constant([1, 1], dtype=tf.int32)], 0\n",
    "        )\n",
    "        return tf.tile(mask, mult)\n",
    "\n",
    "    def call(self, enc_out, target, training):\n",
    "        input_shape = tf.shape(target)\n",
    "        batch_size = input_shape[0]\n",
    "        seq_len = input_shape[1]\n",
    "        causal_mask = self.causal_attention_mask(batch_size, seq_len, seq_len, tf.bool)\n",
    "        target_att = self.self_att(target, target, attention_mask=causal_mask)\n",
    "        target_norm = self.layernorm1(target + self.self_dropout(target_att, training = training))\n",
    "        enc_out = self.enc_att(target_norm, enc_out)\n",
    "        enc_out_norm = self.layernorm2(self.enc_dropout(enc_out, training = training) + target_norm)\n",
    "        ffn_out = self.ffn(enc_out_norm)\n",
    "        ffn_out_norm = self.layernorm3(enc_out_norm + self.ffn_dropout(ffn_out, training = training))\n",
    "        return ffn_out_norm\n",
    "\n",
    "# Customized to add edit_dist metric and training variable.\n",
    "# Reference:\n",
    "# https://www.kaggle.com/code/irohith/aslfr-transformer/notebook\n",
    "# https://www.kaggle.com/code/shlomoron/aslfr-a-simple-transformer/notebook\n",
    "\n",
    "class Transformer(keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_hid=64,\n",
    "        num_head=2,\n",
    "        num_feed_forward=128,\n",
    "        source_maxlen=100,\n",
    "        target_maxlen=100,\n",
    "        num_layers_enc=4,\n",
    "        num_layers_dec=1,\n",
    "        num_classes=60,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.loss_metric = keras.metrics.Mean(name=\"loss\")\n",
    "        self.acc_metric = keras.metrics.Mean(name=\"edit_dist\")\n",
    "        self.num_layers_enc = num_layers_enc\n",
    "        self.num_layers_dec = num_layers_dec\n",
    "        self.target_maxlen = target_maxlen\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.enc_input = LandmarkEmbedding(num_hid=num_hid, maxlen=source_maxlen)\n",
    "        self.dec_input = TokenEmbedding(\n",
    "            num_vocab=num_classes, maxlen=target_maxlen, num_hid=num_hid\n",
    "        )\n",
    "\n",
    "        self.encoder = keras.Sequential(\n",
    "            [self.enc_input]\n",
    "            + [\n",
    "                TransformerEncoder(num_hid, num_head, num_feed_forward)\n",
    "                for _ in range(num_layers_enc)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        for i in range(num_layers_dec):\n",
    "            setattr(\n",
    "                self,\n",
    "                f\"dec_layer_{i}\",\n",
    "                TransformerDecoder(num_hid, num_head, num_feed_forward),\n",
    "            )\n",
    "\n",
    "        self.classifier = layers.Dense(num_classes)\n",
    "\n",
    "    def decode(self, enc_out, target, training):\n",
    "        y = self.dec_input(target)\n",
    "        for i in range(self.num_layers_dec):\n",
    "            y = getattr(self, f\"dec_layer_{i}\")(enc_out, y, training)\n",
    "        return y\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        source = inputs[0]\n",
    "        target = inputs[1]\n",
    "        x = self.encoder(source, training)\n",
    "        y = self.decode(x, target, training)\n",
    "        return self.classifier(y)\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.loss_metric]\n",
    "\n",
    "    def train_step(self, batch):\n",
    "        \"\"\"Processes one batch inside model.fit().\"\"\"\n",
    "        source = batch[0]\n",
    "        target = batch[1]\n",
    "\n",
    "        input_shape = tf.shape(target)\n",
    "        batch_size = input_shape[0]\n",
    "\n",
    "        dec_input = target[:, :-1]\n",
    "        dec_target = target[:, 1:]\n",
    "        with tf.GradientTape() as tape:\n",
    "            preds = self([source, dec_input])\n",
    "            one_hot = tf.one_hot(dec_target, depth=self.num_classes)\n",
    "            mask = tf.math.logical_not(tf.math.equal(dec_target, pad_token_idx))\n",
    "            loss = self.compiled_loss(one_hot, preds, sample_weight=mask)\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        # Computes the Levenshtein distance between sequences since the evaluation\n",
    "        # metric for this contest is the normalized total levenshtein distance.\n",
    "        edit_dist = tf.edit_distance(tf.sparse.from_dense(target),\n",
    "                                     tf.sparse.from_dense(tf.cast(tf.argmax(preds, axis=1), tf.int32)))\n",
    "        edit_dist = tf.reduce_mean(edit_dist)\n",
    "        self.acc_metric.update_state(edit_dist)\n",
    "        self.loss_metric.update_state(loss)\n",
    "        return {\"loss\": self.loss_metric.result(), \"edit_dist\": self.acc_metric.result()}\n",
    "\n",
    "    def test_step(self, batch):\n",
    "        source = batch[0]\n",
    "        target = batch[1]\n",
    "\n",
    "        input_shape = tf.shape(target)\n",
    "        batch_size = input_shape[0]\n",
    "\n",
    "        dec_input = target[:, :-1]\n",
    "        dec_target = target[:, 1:]\n",
    "        preds = self([source, dec_input])\n",
    "        one_hot = tf.one_hot(dec_target, depth=self.num_classes)\n",
    "        mask = tf.math.logical_not(tf.math.equal(dec_target, pad_token_idx))\n",
    "        loss = self.compiled_loss(one_hot, preds, sample_weight=mask)\n",
    "        # Computes the Levenshtein distance between sequences since the evaluation\n",
    "        # metric for this contest is the normalized total levenshtein distance.\n",
    "        edit_dist = tf.edit_distance(tf.sparse.from_dense(target),\n",
    "                                     tf.sparse.from_dense(tf.cast(tf.argmax(preds, axis=1), tf.int32)))\n",
    "        edit_dist = tf.reduce_mean(edit_dist)\n",
    "        self.acc_metric.update_state(edit_dist)\n",
    "        self.loss_metric.update_state(loss)\n",
    "        return {\"loss\": self.loss_metric.result(), \"edit_dist\": self.acc_metric.result()}\n",
    "\n",
    "    def generate(self, source, target_start_token_idx):\n",
    "        \"\"\"Performs inference over one batch of inputs using greedy decoding.\"\"\"\n",
    "        bs = tf.shape(source)[0]\n",
    "        enc = self.encoder(source, training = False)\n",
    "        dec_input = tf.ones((bs, 1), dtype=tf.int32) * target_start_token_idx\n",
    "        dec_logits = []\n",
    "        for i in range(self.target_maxlen - 1):\n",
    "            dec_out = self.decode(enc, dec_input, training = False)\n",
    "            logits = self.classifier(dec_out)\n",
    "            logits = tf.argmax(logits, axis=-1, output_type=tf.int32)\n",
    "            last_logit = logits[:, -1][..., tf.newaxis]\n",
    "            dec_logits.append(last_logit)\n",
    "            dec_input = tf.concat([dec_input, last_logit], axis=-1)\n",
    "        return dec_input\n",
    "\n",
    "class DisplayOutputs(keras.callbacks.Callback):\n",
    "    def __init__(\n",
    "        self, batch, idx_to_token, target_start_token_idx=60, target_end_token_idx=61\n",
    "    ):\n",
    "        \"\"\"Displays a batch of outputs after every 4 epoch\n",
    "\n",
    "        Args:\n",
    "            batch: A test batch\n",
    "            idx_to_token: A List containing the vocabulary tokens corresponding to their indices\n",
    "            target_start_token_idx: A start token index in the target vocabulary\n",
    "            target_end_token_idx: An end token index in the target vocabulary\n",
    "        \"\"\"\n",
    "        self.batch = batch\n",
    "        self.target_start_token_idx = target_start_token_idx\n",
    "        self.target_end_token_idx = target_end_token_idx\n",
    "        self.idx_to_char = idx_to_token\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch % 4 != 0:\n",
    "            return\n",
    "        source = self.batch[0]\n",
    "        target = self.batch[1].numpy()\n",
    "        bs = tf.shape(source)[0]\n",
    "        preds = self.model.generate(source, self.target_start_token_idx)\n",
    "        preds = preds.numpy()\n",
    "        for i in range(bs):\n",
    "            target_text = \"\".join([self.idx_to_char[_] for _ in target[i, :]])\n",
    "            prediction = \"\"\n",
    "            for idx in preds[i, :]:\n",
    "                prediction += self.idx_to_char[idx]\n",
    "                if idx == self.target_end_token_idx:\n",
    "                    break\n",
    "            print(f\"target:      {target_text.replace('-','')}\")\n",
    "            print(f\"prediction: {prediction}\\n\")\n",
    "\n",
    "# Transformer variables are customized from original keras tutorial to suit this dataset.\n",
    "# Reference: https://www.kaggle.com/code/shlomoron/aslfr-a-simple-transformer/notebook\n",
    "\n",
    "batch = next(iter(valid_ds))\n",
    "\n",
    "# The vocabulary to convert predicted indices into characters\n",
    "idx_to_char = list(char_to_num.keys())\n",
    "display_cb = DisplayOutputs(\n",
    "    batch, idx_to_char, target_start_token_idx=char_to_num['<'], target_end_token_idx=char_to_num['>']\n",
    ")  # set the arguments as per vocabulary index for '<' and '>'\n",
    "\n",
    "model = Transformer(\n",
    "    num_hid=200,\n",
    "    num_head=4,\n",
    "    num_feed_forward=400,\n",
    "    source_maxlen = FRAME_LEN,\n",
    "    target_maxlen=64,\n",
    "    num_layers_enc=2,\n",
    "    num_layers_dec=1,\n",
    "    num_classes=62\n",
    ")\n",
    "loss_fn = tf.keras.losses.CategoricalCrossentropy(\n",
    "    from_logits=True, label_smoothing=0.1,\n",
    ")\n",
    "\n",
    "\n",
    "optimizer = keras.optimizers.Adam(0.0001)\n",
    "model.compile(optimizer=optimizer, loss=loss_fn)\n",
    "\n",
    "history = model.fit(train_ds, validation_data=valid_ds, callbacks=[display_cb], epochs=13)\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.legend(['training loss', 'val_loss'])\n",
    "\n",
    "class TFLiteModel(tf.Module):\n",
    "    def __init__(self, model):\n",
    "        super(TFLiteModel, self).__init__()\n",
    "        self.target_start_token_idx = start_token_idx\n",
    "        self.target_end_token_idx = end_token_idx\n",
    "        # Load the feature generation and main models\n",
    "        self.model = model\n",
    "\n",
    "    @tf.function(input_signature=[tf.TensorSpec(shape=[None, len(FEATURE_COLUMNS)], dtype=tf.float32, name='inputs')])\n",
    "    def __call__(self, inputs, training=False):\n",
    "        # Preprocess Data\n",
    "        x = tf.cast(inputs, tf.float32)\n",
    "        x = x[None]\n",
    "        x = tf.cond(tf.shape(x)[1] == 0, lambda: tf.zeros((1, 1, len(FEATURE_COLUMNS))), lambda: tf.identity(x))\n",
    "        x = x[0]\n",
    "        x = pre_process(x)\n",
    "        x = x[None]\n",
    "        x = self.model.generate(x, self.target_start_token_idx)\n",
    "        x = x[0]\n",
    "        idx = tf.argmax(tf.cast(tf.equal(x, self.target_end_token_idx), tf.int32))\n",
    "        idx = tf.where(tf.math.less(idx, 1), tf.constant(2, dtype=tf.int64), idx)\n",
    "        x = x[1:idx]\n",
    "        x = tf.one_hot(x, 59)\n",
    "        return {'outputs': x}\n",
    "\n",
    "tflitemodel_base = TFLiteModel(model)\n",
    "\n",
    "model.save_weights(\"model.h5\")\n",
    "\n",
    "keras_model_converter = tf.lite.TFLiteConverter.from_keras_model(tflitemodel_base)\n",
    "keras_model_converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]#, tf.lite.OpsSet.SELECT_TF_OPS]\n",
    "tflite_model = keras_model_converter.convert()\n",
    "with open('/kaggle/working/model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "infargs = {\"selected_columns\" : FEATURE_COLUMNS}\n",
    "\n",
    "with open('inference_args.json', \"w\") as json_file:\n",
    "    json.dump(infargs, json_file)\n",
    "\n",
    "!zip submission.zip  './model.tflite' './inference_args.json'\n",
    "\n",
    "interpreter = tf.lite.Interpreter(\"model.tflite\")\n",
    "\n",
    "REQUIRED_SIGNATURE = \"serving_default\"\n",
    "REQUIRED_OUTPUT = \"outputs\"\n",
    "\n",
    "with open (\"/kaggle/input/asl-fingerspelling/character_to_prediction_index.json\", \"r\") as f:\n",
    "    character_map = json.load(f)\n",
    "rev_character_map = {j:i for i,j in character_map.items()}\n",
    "\n",
    "found_signatures = list(interpreter.get_signature_list().keys())\n",
    "\n",
    "if REQUIRED_SIGNATURE not in found_signatures:\n",
    "    raise KernelEvalException('Required input signature not found.')\n",
    "\n",
    "prediction_fn = interpreter.get_signature_runner(\"serving_default\")\n",
    "output = prediction_fn(inputs=batch[0][0])\n",
    "prediction_str = \"\".join([rev_character_map.get(s, \"\") for s in np.argmax(output[REQUIRED_OUTPUT], axis=1)])\n",
    "print(prediction_str)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "ASL Fingerspelling Recognition w/ TensorFlow",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (tf-env)",
   "language": "python",
   "name": "tf-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
